"""
RAGç®¡é“æ ¸å¿ƒæ¨¡å—
"""

import chromadb
from chromadb.config import Settings
from typing import Any, Dict, List, Optional, cast
import dspy
import ollama
from sentence_transformers import SentenceTransformer
import numpy as np

from src.config import config
from src.document_loader import Document, DocumentLoader


class ChromaDBRetriever(dspy.Retrieve):
    """Custom ChromaDB Retriever for DSPy"""

    def __init__(
        self,
        collection_name: str = "documents",
        persist_directory: Optional[str] = None,
        embedding_function: Optional[Any] = None,
        k: int = 3,
    ):
        super().__init__(k=k)
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        self.embedding_function = embedding_function

        # Initialize ChromaDB client
        if persist_directory:
            self.chroma_client = chromadb.PersistentClient(
                path=persist_directory,
                settings=Settings(anonymized_telemetry=False),
            )
        else:
            self.chroma_client = chromadb.EphemeralClient()
        self.collection = None

    def _get_collection(self):
        """Get or create collection"""
        if self.collection is None:
            self.collection = self.chroma_client.get_or_create_collection(
                name=self.collection_name, metadata={"hnsw:space": "cosine"}
            )
        return self.collection

    def forward(self, query: str, k: Optional[int] = None, **kwargs) -> List[str]:
        """Retrieve documents from ChromaDB"""
        k = k or self.k
        collection = self._get_collection()

        # Generate embedding for query
        if self.embedding_function is None:
            raise ValueError("embedding_function must be provided")
        query_embedding = self.embedding_function([query])[0]

        # Query collection
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=k,
            include=["documents", "metadatas", "distances"],
        )

        # Format results
        passages = []
        if results and results["documents"]:
            for doc_list in results["documents"]:
                passages.extend(doc_list)

        return passages


class OllamaLM(dspy.LM):
    """Ollamaè¯­è¨€æ¨¡å‹é€‚é…å™¨"""

    def __init__(self, model: Optional[str] = None, base_url: Optional[str] = None):
        selected_model = model or config.ollama_model
        super().__init__(selected_model)
        self.model = selected_model
        self.base_url = base_url or config.ollama_base_url

        # æµ‹è¯•è¿æ¥
        try:
            response = ollama.list()
            print(
                f"âœ… è¿æ¥åˆ° Ollamaï¼Œå¯ç”¨æ¨¡å‹: {[m['name'] for m in response['models']]}"
            )
        except Exception as e:
            raise ConnectionError(f"æ— æ³•è¿æ¥åˆ° Ollama ({self.base_url}): {e}")

    def basic_request(self, prompt: str, **kwargs):
        """åŸºç¡€è¯·æ±‚æ–¹æ³•"""
        response = ollama.generate(
            model=self.model,
            prompt=prompt,
            options={
                "temperature": kwargs.get("temperature", 0.7),
                "max_tokens": kwargs.get("max_tokens", 1000),
                "top_p": kwargs.get("top_p", 0.9),
            },
        )
        return response

    def __call__(
        self,
        prompt: Optional[str] = None,
        max_tokens: Optional[int] = None,
        **kwargs,
    ):
        response = self.basic_request(
            prompt or "",
            max_tokens=max_tokens,
            **kwargs,
        )
        return [response["response"]]


class LocalEmbeddings:
    """æœ¬åœ°åµŒå…¥æ¨¡å‹"""

    def __init__(self, model_name: Optional[str] = None):
        self.model_name = model_name or config.embedding_model
        print(f"åŠ è½½åµŒå…¥æ¨¡å‹: {self.model_name}")
        self.model = SentenceTransformer(self.model_name)

    def embed(self, texts: List[str]) -> List[List[float]]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
        embeddings = self.model.encode(texts, show_progress_bar=False)
        return embeddings.tolist()


class RAGPipeline:
    """RAGç®¡é“ä¸»ç±»"""

    def __init__(self):
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = LocalEmbeddings()

        # åˆå§‹åŒ–ChromaDB
        self.chroma_client = chromadb.PersistentClient(
            path=config.chroma_persist_dir,
            settings=Settings(anonymized_telemetry=False),
        )

        # åˆå§‹åŒ–è‡ªå®šä¹‰ChromaDBæ£€ç´¢å™¨
        self.retriever = ChromaDBRetriever(
            collection_name="documents",
            persist_directory=config.chroma_persist_dir,
            embedding_function=self.embeddings.embed,
            k=config.retrieval_top_k,
        )

        # åˆå§‹åŒ–Ollamaè¯­è¨€æ¨¡å‹
        self.lm = OllamaLM()

        # é…ç½®DSPy
        dspy.configure(lm=self.lm, rm=self.retriever)

        # å®šä¹‰DSPyç­¾å
        self.define_signatures()

    def define_signatures(self):
        """å®šä¹‰DSPyç­¾å"""

        class GenerateAnswer(dspy.Signature):
            """åŸºäºä¸Šä¸‹æ–‡å›ç­”é—®é¢˜"""

            context = dspy.InputField(desc="ç›¸å…³æ–‡æ¡£å†…å®¹")
            question = dspy.InputField(desc="ç”¨æˆ·é—®é¢˜")
            answer = dspy.OutputField(desc="ç®€æ´å‡†ç¡®çš„ç­”æ¡ˆ", format=lambda x: str(x))

        class GenerateAnswerWithReasoning(dspy.Signature):
            """å¸¦æœ‰æ¨ç†è¿‡ç¨‹çš„å›ç­”"""

            context = dspy.InputField(desc="ç›¸å…³æ–‡æ¡£å†…å®¹")
            question = dspy.InputField(desc="ç”¨æˆ·é—®é¢˜")
            reasoning = dspy.OutputField(desc="æ€è€ƒè¿‡ç¨‹")
            answer = dspy.OutputField(desc="æœ€ç»ˆç­”æ¡ˆ")

        self.GenerateAnswer = GenerateAnswer
        self.GenerateAnswerWithReasoning = GenerateAnswerWithReasoning

        # åˆ›å»ºé¢„æµ‹æ¨¡å—
        self.answer_generator = dspy.ChainOfThought(GenerateAnswer)
        self.reasoning_generator = dspy.ChainOfThought(GenerateAnswerWithReasoning)

    def index_documents(
        self, documents: List[Document], collection_name: str = "documents"
    ):
        """ç´¢å¼•æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""

        # åˆ›å»ºæˆ–è·å–é›†åˆ
        collection = self.chroma_client.get_or_create_collection(
            name=collection_name, metadata={"hnsw:space": "cosine"}
        )

        # å‡†å¤‡æ•°æ®
        ids = []
        texts = []
        metadatas = []

        for doc in documents:
            ids.append(doc.id)
            texts.append(doc.content)
            metadatas.append(
                {**doc.metadata, "source": doc.source, "chunk_index": doc.chunk_index}
            )

        # ç”ŸæˆåµŒå…¥
        print("ç”Ÿæˆæ–‡æ¡£åµŒå…¥...")
        embeddings = np.asarray(self.embeddings.embed(texts), dtype=np.float32)

        # æ·»åŠ åˆ°é›†åˆ
        collection.add(
            ids=ids, embeddings=embeddings, metadatas=metadatas, documents=texts
        )

        print(f"âœ… å·²ç´¢å¼• {len(documents)} ä¸ªæ–‡æ¡£å—")
        return collection.count()

    def retrieve(self, query: str, k: Optional[int] = None) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        k = k or config.retrieval_top_k

        # ä½¿ç”¨è‡ªå®šä¹‰æ£€ç´¢å™¨
        passages = self.retriever(query, k=k)

        results = []
        for i, doc in enumerate(passages):
            results.append(
                {
                    "rank": i + 1,
                    "content": doc,
                    "score": 1.0 - (i * 0.1),  # ç®€å•è¯„åˆ†
                    "source": "chromadb",
                }
            )

        return results

    def answer_question(
        self, question: str, use_reasoning: bool = False
    ) -> Dict[str, Any]:
        """å›ç­”é—®é¢˜"""

        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = self.retrieve(question)
        context = "\n\n".join([doc["content"] for doc in retrieved_docs])

        # æˆªæ–­ä¸Šä¸‹æ–‡ä»¥é¿å…è¶…å‡ºé•¿åº¦é™åˆ¶
        if len(context) > config.max_context_length:
            context = context[: config.max_context_length] + "..."

        # ç”Ÿæˆç­”æ¡ˆ
        if use_reasoning:
            pred = self.reasoning_generator(context=context, question=question)
            answer = pred.answer
            reasoning = pred.reasoning
        else:
            pred = self.answer_generator(context=context, question=question)
            answer = pred.answer
            reasoning = None

        return {
            "question": question,
            "answer": answer,
            "reasoning": reasoning,
            "sources": retrieved_docs,
            "context_used": context[:500] + "..." if len(context) > 500 else context,
        }

    def interactive_session(self):
        """äº¤äº’å¼é—®ç­”ä¼šè¯"""
        print("\n" + "=" * 60)
        print("ğŸ¤– æœ¬åœ°RAGåŠ©æ‰‹ (è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º)")
        print("=" * 60)

        while True:
            try:
                question = input("\nâ“ ä½ çš„é—®é¢˜: ").strip()

                if question.lower() in ["quit", "exit", "q"]:
                    print("ğŸ‘‹ å†è§!")
                    break

                if not question:
                    continue

                print("ğŸ§  æ€è€ƒä¸­...")
                result = self.answer_question(question, use_reasoning=True)

                print(f"\nğŸ“ å›ç­”: {result['answer']}")

                if result["reasoning"]:
                    print(f"\nğŸ’­ æ¨ç†è¿‡ç¨‹: {result['reasoning']}")

                print(f"\nğŸ“š å‚è€ƒæ–‡æ¡£:")
                for i, source in enumerate(result["sources"][:3], 1):
                    print(f"  {i}. {source['content'][:200]}...")

            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§!")
                break
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")


class RAGOptimizer:
    """RAGä¼˜åŒ–å™¨"""

    @staticmethod
    def optimize_with_bootstrap(rag_pipeline, train_examples):
        """ä½¿ç”¨BootstrapFewShotä¼˜åŒ–"""

        class RAG(dspy.Module):
            def __init__(self):
                super().__init__()
                self.retrieve = dspy.Retrieve(k=3)
                self.generate_answer = dspy.ChainOfThought(rag_pipeline.GenerateAnswer)

            def forward(self, question):
                passages = cast(List[str], self.retrieve(question))
                context = "\n\n".join(passages) if passages else ""
                return self.generate_answer(context=context, question=question)

        # å®šä¹‰è¯„ä¼°æŒ‡æ ‡
        def validate_answer(example, pred, trace=None):
            # ç®€å•è¯„ä¼°ï¼šæ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åŒ…å«å…³é”®è¯
            gold_answer = example.answer.lower()
            pred_answer = pred.answer.lower()

            # è®¡ç®—é‡å è¯çš„æ¯”ä¾‹
            gold_words = set(gold_answer.split())
            pred_words = set(pred_answer.split())

            if not gold_words:
                return 0

            overlap = len(gold_words.intersection(pred_words)) / len(gold_words)
            return overlap > 0.5  # è‡³å°‘50%é‡å 

        # ä¼˜åŒ–
        teleprompter = dspy.BootstrapFewShot(metric=validate_answer)
        optimized_rag = teleprompter.compile(RAG(), trainset=train_examples)

        return optimized_rag
