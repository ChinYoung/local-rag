"""
è¯­è¨€æ¨¡å‹é€‚é…å™¨æ¨¡å—
"""

import logging
import time
from typing import Optional
import dspy
import ollama
import requests

from src.config import config
from src.utils import retry_on_failure

logger = logging.getLogger(__name__)


class OllamaLM(dspy.LM):
    """Ollamaè¯­è¨€æ¨¡å‹é€‚é…å™¨"""

    def __init__(self, model: Optional[str] = None, base_url: Optional[str] = None):
        selected_model = model or config.ollama_model
        super().__init__(selected_model)
        self.model = selected_model
        self.base_url = base_url or config.ollama_base_url

        # æµ‹è¯•è¿æ¥
        try:
            response = ollama.list()
            print(
                f"âœ… è¿æ¥åˆ° Ollamaï¼Œå¯ç”¨æ¨¡å‹: {[m['name'] for m in response['models']]}"
            )
        except Exception as e:
            raise ConnectionError(f"æ— æ³•è¿æ¥åˆ° Ollama ({self.base_url}): {e}")

    def basic_request(self, prompt: str, **kwargs):
        """åŸºç¡€è¯·æ±‚æ–¹æ³•"""
        response = ollama.generate(
            model=self.model,
            prompt=prompt,
            options={
                "temperature": kwargs.get("temperature", 0.7),
                "max_tokens": kwargs.get("max_tokens", 1000),
                "top_p": kwargs.get("top_p", 0.9),
            },
        )
        return response

    def __call__(
        self,
        prompt: Optional[str] = None,
        max_tokens: Optional[int] = None,
        **kwargs,
    ):
        response = self.basic_request(
            prompt or "",
            max_tokens=max_tokens,
            **kwargs,
        )
        return [response["response"]]


class RemoteOllamaLM(dspy.LM):
    """è¿œç¨‹Ollamaè¯­è¨€æ¨¡å‹é€‚é…å™¨"""

    def __init__(
        self,
        model: str = "",
        base_url: str = "",
        timeout: int = 30,
    ):
        super().__init__(model)
        self.model = model or config.ollama_model
        self.base_url = base_url or config.ollama_base_url
        self.timeout = timeout or config.ollama_timeout

        # é…ç½®ollamaå®¢æˆ·ç«¯
        self._configure_ollama_client()

        # æµ‹è¯•è¿æ¥
        self._test_connection()

    def _configure_ollama_client(self):
        """é…ç½®Ollamaå®¢æˆ·ç«¯"""
        # åˆ›å»ºollamaå®¢æˆ·ç«¯å®ä¾‹
        self.client = ollama.Client(host=self.base_url, timeout=self.timeout)

        logger.info(f"é…ç½®Ollamaå®¢æˆ·ç«¯: {self.base_url}, æ¨¡å‹: {self.model}")

    @retry_on_failure(max_retries=config.ollama_max_retries)
    def _test_connection(self):
        """æµ‹è¯•è¿æ¥å’Œæ¨¡å‹å¯ç”¨æ€§"""
        try:
            logger.info("æµ‹è¯•Ollamaè¿æ¥...")
            logger.info(f"API URL: {self.base_url}")

            # æµ‹è¯•APIè¿æ¥
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            logger.info(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            logger.info(f"å“åº”å†…å®¹: {response.text[:100]}...")
            response.raise_for_status()

            # æ£€æŸ¥æ¨¡å‹
            models = self.client.list()
            logger.debug(f"models response type: {type(models)}")
            logger.debug(f"models response: {models}")

            # Handle different response structures
            available_models = []
            if hasattr(models, "models"):
                # Response object with models attribute
                available_models = [
                    m.get("name", m.get("model", "")) for m in models.models
                ]
            elif isinstance(models, dict) and "models" in models:
                # Dict response
                available_models = [
                    m.get("name", m.get("model", "")) for m in models["models"]
                ]

            # Filter out empty strings
            available_models = [m for m in available_models if m]

            logger.info(f"âœ… è¿æ¥æˆåŠŸ! å¯ç”¨æ¨¡å‹: {available_models}")

            # æ£€æŸ¥æ‰€éœ€æ¨¡å‹æ˜¯å¦å¯ç”¨
            if self.model not in available_models:
                logger.warning(
                    f"âš ï¸  æ¨¡å‹ {self.model} æœªæ‰¾åˆ°ï¼Œå¯ç”¨æ¨¡å‹: {available_models}"
                )
                logger.warning(f"å»ºè®®è¿è¡Œ: ollama pull {self.model}")

            # æ˜¾ç¤ºæ¨¡å‹è¯¦æƒ…
            try:
                model_info = self.client.show(self.model)
                logger.info(
                    f"ğŸ“‹ æ¨¡å‹è¯¦æƒ…: {model_info.get('modelfile', 'N/A')[:100]}..."
                )
            except:
                logger.warning(f"æ— æ³•è·å–æ¨¡å‹ {self.model} çš„è¯¦ç»†ä¿¡æ¯")

        except requests.exceptions.ConnectionError as e:
            error_msg = (
                f"âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡å™¨ {self.base_url}\n"
                f"   è¯·æ£€æŸ¥:\n"
                f"   1. æœåŠ¡å™¨åœ°å€æ˜¯å¦æ­£ç¡®\n"
                f"   2. æœåŠ¡å™¨æ˜¯å¦æ­£åœ¨è¿è¡Œ\n"
                f"   3. é˜²ç«å¢™/ç½‘ç»œè®¾ç½®\n"
                f"   4. å¦‚æœéœ€è¦è®¤è¯ï¼Œè¯·è®¾ç½® OLLAMA_USERNAME å’Œ OLLAMA_PASSWORD"
            )
            logger.error(error_msg)
            raise ConnectionError(error_msg) from e
        except Exception as e:
            logger.error(f"è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            raise

    @retry_on_failure(max_retries=config.ollama_max_retries)
    def basic_request(self, prompt: str | None = None, **kwargs):
        """åŸºç¡€è¯·æ±‚æ–¹æ³• - æ”¯æŒè¿œç¨‹è°ƒç”¨"""
        if prompt is None:
            prompt = ""
        try:
            response = self.client.generate(
                model=self.model,
                prompt=prompt,
                options={
                    "temperature": kwargs.get("temperature", 0.7),
                    "max_tokens": kwargs.get("max_tokens", 1000),
                    "top_p": kwargs.get("top_p", 0.9),
                    "top_k": kwargs.get("top_k", 40),
                },
                stream=False,
            )
            return response
        except ollama.ResponseError as e:
            logger.error(f"Ollama APIé”™è¯¯: {e.error}")
            if "model not found" in str(e.error).lower():
                raise ValueError(f"æ¨¡å‹ {self.model} æœªæ‰¾åˆ°ï¼Œè¯·å…ˆåœ¨æœåŠ¡å™¨ä¸Šå®‰è£…")
            raise
        except Exception as e:
            logger.error(f"è¯·æ±‚å¤±è´¥: {e}")
            raise

    def __call__(
        self,
        prompt: Optional[str] = None,
        messages: Optional[list] = None,
        **kwargs,
    ):
        """è°ƒç”¨è¯­è¨€æ¨¡å‹"""
        start_time = time.time()

        # Log all parameters received
        logger.info(f"LM __call__ è¢«è°ƒç”¨")
        logger.info(
            f"  - promptç±»å‹: {type(prompt)}, é•¿åº¦: {len(prompt) if prompt else 0}"
        )
        logger.info(f"  - messages: {messages}")
        logger.info(f"  - kwargs: {list(kwargs.keys())}")

        # DSPy may pass the prompt in different ways
        actual_prompt = prompt

        # Check if prompt is in kwargs
        if not actual_prompt and "prompt" in kwargs:
            actual_prompt = kwargs.pop("prompt")
            logger.info(
                f"ä»kwargsä¸­è·å–prompt, é•¿åº¦: {len(actual_prompt) if actual_prompt else 0}"
            )

        # Check if messages format is used
        if not actual_prompt and messages:
            # Convert messages to prompt
            actual_prompt = "\n".join(
                [m.get("content", "") for m in messages if isinstance(m, dict)]
            )
            logger.info(f"ä»messagesè½¬æ¢prompt, é•¿åº¦: {len(actual_prompt)}")

        if not actual_prompt:
            logger.error("âš ï¸ æ²¡æœ‰æ”¶åˆ°æœ‰æ•ˆçš„prompt!")
            logger.error(f"å®Œæ•´kwargs: {kwargs}")
            # Return empty to avoid crash
            return [""]

        logger.info(f"æœ€ç»ˆprompté•¿åº¦: {len(actual_prompt)}")
        logger.debug(f"Promptå†…å®¹: {actual_prompt[:200]}...")

        try:
            response = self.basic_request(actual_prompt, **kwargs)
            elapsed = time.time() - start_time

            # Extract response text
            response_text = response.get("response", "")
            logger.info(f"æ¨¡å‹å“åº”æ—¶é—´: {elapsed:.2f}s, å“åº”é•¿åº¦: {len(response_text)}")
            logger.debug(f"å“åº”å†…å®¹: {response_text[:200]}...")

            if not response_text:
                logger.warning("âš ï¸ æ¨¡å‹è¿”å›ç©ºå“åº”!")

            return [response_text]
        except Exception as e:
            logger.error(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
            raise

    def stream_generate(self, prompt: str, **kwargs):
        """æµå¼ç”Ÿæˆï¼ˆå¯é€‰åŠŸèƒ½ï¼‰"""
        try:
            response = self.client.generate(
                model=self.model,
                prompt=prompt,
                stream=True,
                options={
                    "temperature": kwargs.get("temperature", 0.7),
                    "max_tokens": kwargs.get("max_tokens", 1000),
                },
            )

            for chunk in response:
                if chunk.get("response"):
                    yield chunk["response"]
        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            raise
